# MUFJ Data Science Champion Ship
総合的感想
- 文章は多言語で非常に汚く，前処理をどれぐらい頑張れるかで結果が決まってきているだろう
- 文とテーブルデータを同時に扱ったのは初めてであり，モデリング面でももっと工夫ができそう
- CV, LB, PLB は相関が強く，頑張り甲斐があるコンペであった

できたこと・工夫したこと
- システム
    - 実験番号ごとにコードを管理する仕組み
        - 理由：ブランチ切って管理する場合，各実験ごとにいつのブランチのものか覚えておく必要があり，再現性が得づらいため
        - 方法1：main.py を動かせば次の実験番号のディレクトリを生成し，コードを複製して実行するようにした
        - 方法2：グリッドサーチ(gs)の場合も同様に扱われるようにした
        - 方法3：debug 時は新しいディレクトリ生成したくないので，d ディレクトリに結果を保存するようにした
- 前処理上の工夫
    - 非英語センテンスの削除
        - 理由1：多言語データであることから，最初は多言語対応モデルの xlm-roberta 系統や bert-multilingual 系統を用いていたが，精度があまり出なかったため
        - 理由2：非英語を含んでいるデータは全体の7%であり，非英語だけで構成されているデータはさらに希少だったため
        - 理由3：英語と非英語混在の場合，英語文が非英語文の翻訳的立ち位置になっていう例が半数程度あったため
        - 方法：文単位（ピリオド，エクスクラメーション，クエスチョンなどを文の区切りと判断）ごとに is_ascii メソッドで非英語が含まれているかどうか判定
    - テーブルデータを文の最初に結合
        - 理由：文の BERT 出力とテーブルデータを合わせて MLP 通すモデルより精度が良かったため
        - 方法1：goal 変数はハイフンの後ろ側の値を 1/1000 して文字列化（数字が大きすぎると，複数の数字列に割れて埋め込まれてしまうため）
        - 方法2：各変数の間は sep_token でつなげた
    - 分類ラベル・カテゴリーに応じた階層化 k-fold
        - 理由：fold ごとにカテゴリーにばらつきがあると，OOF の結果がばらつきやすいと考えたため
    - 文中のホームページアドレスや twitter リンク，動画の埋め込みによって生じる文章など，不要な単語・文の置換・削除
    - HTML 言語の削除
- 学習上の工夫
    - 学習係数を前の層ほど小さく設定できるようにした
        - 理由：前の層ほどタスク非依存の本質的な情報を学習していると考えられるため
        - 結果：影響はあまり検証できていない
    - 敵対的学習（FGM,AWP）
        - 結果：FGM を用いたモデルが軒並み良く，0.01ほど改善
    - 擬似ラベリング
        - 結果：単体では精度改善の効果はあまり見られなかったが，モデルアンサンブルと組み合わせると 0.003 ほど上昇
    - テーブルデータ用の MLP と BERT 出力を組み合わせた予測
        - 結果：テーブルデータを文章化して BERT 単体モデルに 0.04 ほど劣敗
    - モデルアンサンブル
        - 結果：単体モデルのベストに比べ，0.01ほど上昇
        
        
できなかったこと・改善していきたいこと
- システム
    - train 後に test に移行しない事例が何度か見られた
        - run_bert 内で train 関数と test 関数を順番に実行する仕組みにしていたが，train 関数終了（confusion matrix 描画）後に何故か test 関数を実行せず永遠に train 関数内にいることが何度かあった
        - 起こる場合と起こらない場合があり，何が原因なのか全く分からなかった
        - 1週間対応できないときに，グリッドサーチの最初で止まっていたのはかなりショックだった（時間の大幅損失）
        - 対応：train, test を独立のプロセスとしてからは一応起こらなくなった
        - 改善案1：学習終了後に slack やメールで通知が届くようにし，遅滞している場合に迅速に対応できるようにする
        - 改善案2：見積もり時間の2倍を超えても終わらない場合に train プロセスを強制的に kill する仕組みを導入する
- 前処理
    - 単語単位で非英語言語を除去したかった
        - 理由1：「地震（earthquake）」のように「非英語（英語）」という形式が，英語・非英語混在の3割ほどで見られたため
        - 理由2（しなかった理由）：欧州言語だとアクセント記号が含まれていない単語では，英語/非英語の判定が難しいため
    - 非英語文は英語に翻訳しても良かった
        - 理由1：非英語データのうち，非英語だけで構成されているデータも半数ほどあったため
        - 理由2:xlm-roberta 系統だと非英語は希少単語であり学習が難しくても，一度翻訳して仕舞えば deberta-v3 などに落とせるため
- 学習
    - FGM の学習時間が長い
        - AWP と操作自体はあまり変わらないはずなのに異様に長いため，コード面で時間削減したい
    - モデルアンサンブルにおいて総和が1の制約が効いていない
        - 理由1：Nelder-Mead だと制約が効かせられないため
        - 理由2：weight を総和で割って規格化しながら回すと却って結果が悪くなる
        - 改善案1：多クラス分類のように0,1それぞれの確率の重み付き和で大きい方を選択するようにすれば良い（閾値で区切って0,1判定しているので総和を1に制限したかった）
    - 自信過剰の対策
        - 今回は検証用データにおける予測確率（信頼度）と各ビンごとの正解率が概ね対応していた
        - 改善案1：一般的には対応していない場合もあるため，キャリブレーションの導入
    - 文を一切使わない予測モデル
        - RF で簡単に構成できそうなので，ベンチマークとして作っておけば良かった
        - 理由：文情報を入れることによる効果を定量的に評価できる
    - ハイパラ自動探索
        - 理由1(しなかった理由)：モデルの選択などカテゴリカル変数が多く，探索に時間がかかりそうだったため
        - 理由2(しなかった理由)：optuna でやる場合，comet log との両立など考慮すべきことが多く，コーディングの時間が取れなかったため
    - best, last の2つの結果を保存
        - 理由：best model と last mmodel のどちらを使うべきか曖昧であり，両者の結果を保存しておいた方が後で簡単に切り替えられるため
    - 擬似ラベリングの効果検証
        - 前とどれぐらい変わったかを途中の学習段階においても示す仕組み
    - MLM
        - 導入しなかった理由：特定分野の学習ではなく，学習時間に対するコストパフォーマンスが低そうと判断したため
- 可視化
    - debug の確認リストを作っても良かった
        - 理由：モデルが動くだけでは確認できないサイレントエラーを検知するためにも，debug 時に確認すべき項目を考え出力した方が時間を有効活用できるため
    - CLS トークンが効いている単語を可視化する仕組み
        - 理由1：学習結果の解釈から，次のモデル案に繋げられるため
        - 理由2(しなかった理由)：どの層のどのヘッドを見るべきか分からなかったため
    
        


## Score
|id|CV|LB|PLB|memo|
|----|----|----|----|----|
|1|0.659|0.641|0.651|miss in FL. almost predictions are 1|
|45|0.769|0.783|0.789|deberta-large, 3e-5, loss transition is up-down|
|50|0.803|0.815|0.811|deberta-v3-base, 2e-5, sc=None|
|51|0.808|0.820|0.818|deberta-v3-base, 2e-5, do=0, sc=None|
|65|0.810|0.824|0.812|deberta-v3-base, 2e-5, do=0.15, FGM|
|e5|0.817|0.829|0.825|65,65p1,98,98p1,104|
|e13|0.818|0.830|0.823|65,65p1,98,98p1,104,104p1|
|e18|0.818|0.829|0.825|65,65p1,98,98p1,104,104p1,63|
|e19|0.818|0.827|0.823|65,65p1,98,98p1,104,104p1,63,51|
|e23|0.818|0.828|0.823|65,65p1,98,98p1,104,104p1,weight normalized|

## Computation Time
kfold=5 + all
|model|at|ep|bs|time|memory|
|----|----|----|----|----|----|
|deberta-v3-base|None|4|16|2:50|19803|
||AWP|4|16|3:50||
||FGM|4|16|4:50||
|deberta-base|None|4|16|3:20||
|deberta-large|3|8|4:00||
|roberta-base|None|4|16|2:10|13543|
|roberta-large|None|3|8|3:00||
|xlm-roberta-base|None|4|16|2:10|16311|